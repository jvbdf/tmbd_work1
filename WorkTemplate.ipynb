{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "375aacc5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Técnicas Matemáticas para Big Data - Project NN?\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b47c68c",
   "metadata": {},
   "source": [
    "GROUP NN:\n",
    "- Student 1 - Nº 106078 - 33.33% Work Participation\n",
    "- Student 2 - Nº xxxxx - ??% Work Participation\n",
    "- Student 3 - Nº xxxxx - ??% Work Participation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66fb045",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## 1. Introduction to the problem of study [1,0 valor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec2db1f",
   "metadata": {},
   "source": [
    "### Estimating Unique Customers with the HyperLogLog Algorithm\n",
    "\n",
    "We aim to process e-commerce purchase data chronologically, simulating a **data stream** using real sales data obtained from the [Online Retail Dataset – UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/352/online+retail).\n",
    "\n",
    "The dataset contains all transactions that occurred between **01/12/2010 and 09/12/2011** for a **UK-based non-store online retailer**.\n",
    "\n",
    "Each new event (purchase) is added to the data stream, ordered by its corresponding **timestamp**.  \n",
    "The main objective is to **estimate the number of unique customers** who made a purchase in real time.\n",
    "\n",
    "With continuous technological advancements and the ever-growing volume of users and data, companies must monitor customer activities in **real time**. However, the massive amount of data generated requires both **high processing capacity** and **speed**.  \n",
    "\n",
    "Processing data on a large scale is a current technological challenge, as it demands the ability to deliver accurate statistics, analytics, and insights almost instantaneously, without compromising system performance or precision.\n",
    "\n",
    "In this context, the problem addressed focuses on **estimating the number of unique buyers** in an online sales platform as new purchase events continuously arrive.  \n",
    "Each transaction in the dataset represents a sale made by a specific customer at a given moment, allowing us to simulate a **data stream** where events are processed in chronological order.\n",
    "\n",
    "The goal is to estimate, in real time, **how many distinct customers have made purchases** up to any given point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41af2342",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## 2. Brief and general description of the approach and methods used [1,5 valor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504b7b8d",
   "metadata": {},
   "source": [
    "\n",
    "### Motivation\n",
    "\n",
    "Processing massive amounts of data by storing them in a database and iterating over each record would require **significant computational resources**, making such an approach infeasible.\n",
    "\n",
    "In a traditional setup, counting distinct customers would require storing every `CustomerID` in a database or cache and performing periodic scans as new records arrive.  \n",
    "Although this approach provides exact results, it becomes **inefficient** when applied to large-scale datasets.\n",
    "\n",
    "The amount of memory required grows **linearly** with the number of customers, and recomputing distinct counts from stored data can become **very expensive** in terms of both time and system resources.\n",
    "\n",
    "### Solution: The HyperLogLog Algorithm (HLL)\n",
    "\n",
    "To perform this task efficiently, we implemented the **probabilistic algorithm [HyperLogLog(HLL)](https://en.wikipedia.org/wiki/HyperLogLog)**, designed to **estimate the cardinality** (number of distinct elements) in large datasets.\n",
    "\n",
    "The HyperLogLog algorithm can estimate cardinalities on the order of 10⁹ elements with a **relative error below 2%**, while using only about **1.5 kB of memory**.\n",
    "\n",
    "### Methodology\n",
    "\n",
    "In **real-time analytics systems**, where millions of events may arrive every minute, storing and maintaining exact counts of unique elements quickly becomes impractical.\n",
    "\n",
    "Instead of storing every individual customer identifier, HyperLogLog(HLL):\n",
    "\n",
    "1. **Applies a hash function** to each incoming `CustomerID` into a value;  \n",
    "2. **Updates a set of registers** represent the overall distribution of those hashes.\n",
    "\n",
    "This let the algorithm estimate the **number of unique costumers** observed so far, with a small and controllable margin of error determined by the **precision parameter `p`**.\n",
    "\n",
    "### Advantages\n",
    "\n",
    "The main advantage of HyperLogLog lies in its **memory efficiency** and **processing speed**:\n",
    "\n",
    "- Uses a **fixed amount of memory**, regardless of the stream size;  \n",
    "- Processes each incoming event in **constant or near-constant time**;  \n",
    "- Maintains **high accuracy with minimal computational cost**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e324615",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## 3. Brief History and literature review of the problem and methods/algorithms [1,5 valor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c003888c",
   "metadata": {},
   "source": [
    "The problem of efficiently counting distinct elements in large datasets has long been a central challenge in data processing and database systems.  \n",
    "Early approaches relied on exact methods, which required storing all unique identifiers in memory or on disk.  \n",
    "While these techniques are straightforward, their **space complexity grows linearly** with the number of unique elements, making them impractical for large-scale or streaming environments where data arrives continuously and in massive volumes.\n",
    "\n",
    "In the early 1980s, **Philippe Flajolet and G. Nigel Martin** introduced the [Flajolet–Martin (FM) algorithm](https://algo.inria.fr/flajolet/Publications/src/FlMa85.pdf), one of the first probabilistic algorithms for approximate distinct counting.  \n",
    "Their method relied on hashing each element and observing the position of the least significant 1-bit in the binary representation of the hash values.  \n",
    "The expected position of this bit could be used to estimate the logarithm of the cardinality of the dataset.  \n",
    "Although revolutionary, the original FM algorithm exhibited high variance and limited accuracy for large datasets.\n",
    "This original algorithm used only a global counter to store the highest value of the first bit position, 1, that is, the highest number of consecutive leading zeros observed in the hashes.\n",
    "The problem is that this variable had a very high variance. Therefore, just a single extreme value can drastically change the result.\n",
    "\n",
    "\n",
    "To improve the precision of probabilistic counting, **Flajolet et al.** later proposed the **LogLog** algorithm and, subsequently, the [HyperLogLog (HLL)](https://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf) algorithm in 2007.  \n",
    "The HyperLogLog introduced a refined estimator based on the **harmonic mean** of multiple sub-estimators (called registers) and incorporated a **bias-correction constant** through the concept of **stochastic averaging**.  \n",
    "This design reduced the **relative standard error (RSE)** to approximately \\(1.04 / \\sqrt{m}\\), where \\(m = 2^p\\) is the number of registers.  \n",
    "Instead of maintaining a single global counter, the hash space is divided into \\(m\\) disjoint subsets (or buckets), and an independent estimate is computed for each subset.\n",
    "\n",
    "The HyperLogLog algorithm thus achieved an elegant balance between **accuracy**, **computational efficiency**, and **memory usage**.\n",
    "\n",
    "HLL thus provided an elegant balance between **accuracy**, **computational efficiency**, and **memory usage**.\n",
    "\n",
    "Over time, the HyperLogLog algorithm became one of the most widely adopted techniques for large-scale analytics.  \n",
    "It is currently implemented in major data systems such as **Google BigQuery**, **Redis**, **Apache Spark**, and **PostgreSQL**, where it supports real-time analytics and streaming queries.  \n",
    "Its **merge** property — the ability to combine multiple estimators into a single global one — makes it particularly suitable for **distributed architectures** and **parallel computation** environments.\n",
    "\n",
    "Beyond web analytics and online systems, HyperLogLog is used in **network monitoring**, **fraud detection**, **ad impression counting**, and **data deduplication**.  \n",
    "Its efficiency lies in providing **near-constant memory usage** and **fast incremental updates**, allowing accurate estimation of distinct elements in data streams that would otherwise be too large to store or process exactly.\n",
    "\n",
    "HyperLogLog represents a key milestone in the evolution of probabilistic counting algorithms, combining solid mathematical foundations with proven scalability and practical relevance in modern Big Data applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4889a980",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## 4. About the main method/algorithm used [1,5 valor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5199dc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Core principle\n",
    "\n",
    "The algorithm uses hash functions to obtain a binary representation of the data and the key insight behind HyperLogLog is that the position of the **first 1-bit** in the representation of a uniformly distributed hash value carries information about the magnitude of the dataset’s cardinality.  \n",
    "If the dataset is large, it becomes increasingly likely to observe hash values with long sequences of leading zeros.  \n",
    "By analyzing the distribution of these positions over many independent hash samples, the algorithm can infer the total number of unique elements.\n",
    "\n",
    "### Data structure\n",
    "\n",
    "HyperLogLog maintains an array of \\( m = 2^p \\) **registers**(buckets), each of which stores a integer that represents the maximum number of leading zeros observed among the hash values assigned to that register.  \n",
    "The parameter \\( p \\) determines both the **number of registers** and the **precision** of the estimation:\n",
    "- A higher \\( p \\) means more registers (greater accuracy) but slightly higher memory usage.\n",
    "- A lower \\( p \\) reduces memory but increases the standard error.\n",
    "\n",
    "The **relative standard error (RSE)** of the algorithm is approximately:\n",
    "\n",
    "\\[\n",
    "\\text{RSE} = \\frac{1.04}{\\sqrt{m}} = \\frac{1.04}{\\sqrt{2^p}}\n",
    "\\]\n",
    "\n",
    "Thus, each increment of one bit in \\( p \\) reduces the error by roughly \\( \\sqrt{2} \\).\n",
    "\n",
    "### Algorithm procedure\n",
    "\n",
    "1. **Hashing:**  \n",
    "   Each element \\( x \\) is passed through a hash function (e.g., SHA-1 or SHA-256) to produce a 64-bit value \\( h(x) \\).  \n",
    "   This ensures a uniform and random distribution across the hash space.\n",
    "\n",
    "2. **Register selection:**  \n",
    "   The first \\( p \\) bits of the hash are used to select which register \\( j \\) to update.  \n",
    "   This splits the hash space into \\( 2^p \\) buckets.\n",
    "\n",
    "3. **Zero counting:**  \n",
    "   The remaining bits of the hash are used to count the number of leading zeros \\( \\rho(x) \\) after the prefix.  \n",
    "   The corresponding register \\( M[j] \\) is updated as:\n",
    "\n",
    "   \\[\n",
    "   M[j] = \\max(M[j], \\rho(x))\n",
    "   \\]\n",
    "\n",
    "4. **Estimation:**  \n",
    "   After all elements are processed, the algorithm computes the harmonic mean of the register values:\n",
    "\n",
    "   \\[\n",
    "   E = \\alpha_m \\cdot m^2 \\cdot \\left( \\sum_{j=1}^{m} 2^{-M[j]} \\right)^{-1}\n",
    "   \\]\n",
    "\n",
    "   where \\( \\alpha_m \\) is a **bias correction constant** that depends on \\( m \\):\n",
    "   - \\( \\alpha_{16} = 0.673 \\)\n",
    "   - \\( \\alpha_{32} = 0.697 \\)\n",
    "   - \\( \\alpha_{64} = 0.709 \\)\n",
    "   - For \\( m \\geq 128 \\): \\( \\alpha_m = 0.7213 / (1 + 1.079/m) \\)\n",
    "\n",
    "### Corrections and refinements\n",
    "\n",
    "The HyperLogLog algorithm applies different estimation formulas depending on the observed cardinality range.  \n",
    "This prevents systematic bias that may occur when the dataset is either too small or too large relative to the number of registers.  \n",
    "The algorithm therefore operates under **three distinct regimes**:\n",
    "\n",
    "1. **Small-range correction (Linear Counting):**  \n",
    "   When the raw estimate \\( E \\) is **less than \\( 2.5m \\)**, the HyperLogLog tends to overestimate due to a large number of empty registers.  \n",
    "   In this regime, the algorithm switches to **linear counting**, using the formula:\n",
    "\n",
    "   \\[\n",
    "   E' = m \\cdot \\ln\\left(\\frac{m}{V}\\right)\n",
    "   \\]\n",
    "\n",
    "   where \\( V \\) is the number of registers with value 0.  \n",
    "   This correction provides more accurate results when the number of distinct elements is small compared to the number of registers.\n",
    "\n",
    "2. **Intermediate-range (Standard Estimation):**  \n",
    "   When the cardinality is within a normal operating range — typically between \\( 2.5m \\) and \\( (1/30) \\times 2^{32} \\) —  \n",
    "   the **raw HyperLogLog estimate** is sufficiently accurate and no correction is required:\n",
    "\n",
    "   \\[\n",
    "   E' = E = \\alpha_m \\cdot m^2 \\cdot \\left( \\sum_{j=1}^{m} 2^{-M[j]} \\right)^{-1}\n",
    "   \\]\n",
    "\n",
    "   This is the most common regime in practical applications, where the estimator achieves its theoretical relative standard error of \\( 1.04 / \\sqrt{m} \\).\n",
    "\n",
    "3. **Large-range correction (Bias Reduction):**  \n",
    "   When the estimated value \\( E \\) exceeds approximately \\( (1/30) \\times 2^{32} \\),  \n",
    "   the algorithm may **underestimate** due to hash collisions approaching the 32-bit space limit.  \n",
    "   In this case, a **large-range correction** is applied:\n",
    "\n",
    "   \\[\n",
    "   E' = -2^{32} \\cdot \\ln\\left(1 - \\frac{E}{2^{32}}\\right)\n",
    "   \\]\n",
    "\n",
    "   This compensates for the nonlinearity that arises when the hash space becomes saturated and the probability of collisions increases.\n",
    "\n",
    "These three regimes together ensure that HyperLogLog remains accurate across a wide range of dataset sizes — from a few hundred distinct elements to billions — using the same fixed memory footprint.  \n",
    "This adaptability is one of the main reasons why HyperLogLog has become the de facto standard for scalable cardinality estimation in data stream processing.\n",
    "\n",
    "\n",
    "### Merge property\n",
    "\n",
    "A major advantage of HyperLogLog is its **mergeability**.  \n",
    "Given two HLLs \\( A \\) and \\( B \\) built from different subsets of the data, their union can be computed simply as:\n",
    "\n",
    "\\[\n",
    "M_{\\text{merged}}[j] = \\max(M_A[j], M_B[j])\n",
    "\\]\n",
    "\n",
    "This operation is associative and idempotent, allowing partial counts from distributed systems to be combined efficiently without double-counting elements.  \n",
    "This makes HyperLogLog highly suitable for **parallel** and **distributed architectures**, such as those used in real-time analytics and Big Data pipelines.\n",
    "\n",
    "### Complexity and memory efficiency\n",
    "\n",
    "The algorithm runs in **O(1)** time per insertion and requires **O(2^p)** memory, which is constant with respect to the total number of elements processed.  \n",
    "Typical configurations (e.g., \\( p = 14 \\)) require only a few kilobytes of memory and achieve a relative error below 1%.  \n",
    "This balance between accuracy, speed, and memory efficiency is what makes HyperLogLog one of the most practical algorithms for streaming distinct counting tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52795c65",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## 5. Python imports and global configurations [0,5 valor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0f3deb",
   "metadata": {},
   "source": [
    "### Install and import the necessary libraries to compute the Bayesian Network and perform other methods  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5177dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.10.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: pyparsing>=3 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ucimlrepo in /usr/local/lib/python3.10/dist-packages (0.0.7)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2025.10.5)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2.3.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=1.0.0->ucimlrepo) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.2.6)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "%pip install pandas\n",
    "%pip install matplotlib\n",
    "%pip install ucimlrepo\n",
    "\n",
    "import hashlib\n",
    "import matplotlib\n",
    "import math\n",
    "from ucimlrepo import fetch_ucirepo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb739aae",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## 6. Dataset and variables explanation [1,5 valor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed92534",
   "metadata": {},
   "source": [
    "The dataset used in this project is the **Online Retail Dataset** from the UCI Machine Learning Repository.  \n",
    "It contains **541,910 transactions** recorded by a United Kingdom–based online gift retailer between **December 2010 and December 2011**.  \n",
    "Each record represents an individual product purchased within an invoice, providing detailed information about sales activity across multiple countries and customers.\n",
    "\n",
    "### Structure of the dataset\n",
    "\n",
    "The dataset includes the following main variables:\n",
    "\n",
    "| Variable | Description |\n",
    "|-----------|-------------|\n",
    "| **InvoiceNo** | Unique identifier for each invoice. Cancellations are indicated by invoices starting with the letter “C”. |\n",
    "| **StockCode** | Unique code assigned to each product. |\n",
    "| **Description** | Text description of the product. |\n",
    "| **Quantity** | Number of units purchased (negative values indicate product returns). |\n",
    "| **InvoiceDate** | Timestamp of the transaction (date and time when the invoice was issued). |\n",
    "| **UnitPrice** | Price per unit of the product in pounds sterling (£). |\n",
    "| **CustomerID** | Unique identifier for the customer who made the purchase. |\n",
    "| **Country** | Country of the customer. |\n",
    "\n",
    "### Data preparation\n",
    "\n",
    "Before the data can be used for streaming analysis, several preprocessing steps are applied:\n",
    "\n",
    "1. **Removal of missing values:**  \n",
    "   Some transactions lack a `CustomerID`.  \n",
    "   These rows are excluded since the study focuses on identifying **unique buyers**, and missing identifiers cannot be used for distinct counting.\n",
    "\n",
    "2. **Filtering valid sales:**  \n",
    "   Records with negative `Quantity` correspond to product returns.  \n",
    "   Although these events may be relevant in other analyses, they do not represent a new purchase.  \n",
    "   Thus, only transactions with `Quantity > 0` are kept to represent genuine purchase events.\n",
    "\n",
    "3. **Sorting by time:**  \n",
    "   The dataset is sorted by `InvoiceDate` to simulate a **chronological data stream**, where each transaction is processed in the order it occurred.\n",
    "\n",
    "4. **Type normalization:**  \n",
    "   The `CustomerID` is treated as a string to ensure consistency.\n",
    "\n",
    "5. **Feature selection:**  \n",
    "   For this study, only the columns `InvoiceDate`, `CustomerID`, and `Country` are required.  \n",
    "   Other variables (e.g., `StockCode`, `UnitPrice`) are kept only for potential extensions, such as analyzing unique products or merging HLLs across regions.\n",
    "\n",
    "### Motivation for using this dataset\n",
    "\n",
    "The Online Retail dataset is useful for this project for several reasons:\n",
    "\n",
    "- It provides **real-world transactional data**, where each row naturally represents a **streaming event**.  \n",
    "- The presence of **timestamps** (`InvoiceDate`) enables the simulation of a **real-time data stream**, allowing HyperLogLog to be tested in an online setting.  \n",
    "- The inclusion of multiple countries also allows demonstration of HLL’s **merge** property — combining partial estimators built per region into a global one.\n",
    "\n",
    "### 6.4. Variables used in this study\n",
    "\n",
    "The main variable analyzed is **`CustomerID`**, which identifies each buyer.  \n",
    "The goal is to estimate, in real time, the number of **unique customers** who have made purchases up to any given point in time.\n",
    "\n",
    "Other variables are used for contextual purposes:\n",
    "- **`InvoiceDate`** is used to order the transactions chronologically, forming the basis of the data stream.  \n",
    "- **`Country`** allows the study of customer diversity across geographic regions and serves as a grouping variable for distributed HLL estimation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5703771",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## 7. Main code as possible solution to the problem [1,5 valor] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c735dd50",
   "metadata": {},
   "source": [
    "**Hashing**  \n",
    "Each element is processed through a deterministic hash function (`sha1` or \n",
    "`sha256`). Only the first 64 bits of the hash are used for performance and \n",
    "consistency.\n",
    "\n",
    "**Register Selection**  \n",
    "The first *p* bits of the hash determine which register (bucket) is updated.  \n",
    "This means the structure has `m = 2^p` registers.\n",
    "\n",
    "**Leading Zero Count (Rho Value)**  \n",
    "The remaining bits of the hash are inspected to count the number of leading\n",
    "zeros, plus one:\n",
    "\n",
    "$$\n",
    "\\rho = \\text{count leading zeros}(w) + 1\n",
    "$$\n",
    "\n",
    "This value statistically represents how \"rare\" a hash position is.\n",
    "\n",
    "**Register Update Rule**  \n",
    "Registers store **only the maximum** rho value observed for their index:\n",
    "\n",
    "$$\n",
    "M[j] = \\max(M[j], \\rho)\n",
    "$$\n",
    "\n",
    "**Harmonic Mean Estimation**  \n",
    "After processing the stream, the estimate is computed using:\n",
    "\n",
    "$$\n",
    "E = \\alpha_m \\cdot \\frac{m^2}{\\sum_{j=1}^{m} 2^{-M[j]}}\n",
    "$$\n",
    "\n",
    "This formula uses the **harmonic mean** of the bucket contributions and the \n",
    "empirical correction factor:\n",
    "\n",
    "$$\n",
    "\\alpha_m = \n",
    "\\begin{cases}\n",
    "0.673 & m = 16 \\\\\n",
    "0.697 & m = 32 \\\\\n",
    "0.709 & m = 64 \\\\\n",
    "\\frac{0.7213}{1 + 1.079/m} & m \\ge 128\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Small Range Correction**  \n",
    "For datasets where the estimate is relatively small and many registers remain \n",
    "empty, the linear counting correction improves accuracy:\n",
    "\n",
    "$$\n",
    "E' = m \\cdot \\ln\\left(\\frac{m}{V}\\right)\n",
    "\\quad \\text{if } E \\le 2.5m\n",
    "$$\n",
    "\n",
    "Where `V` is the number of registers with value zero.\n",
    "\n",
    "**Final Output**  \n",
    "The method `estimate()` returns a floating-point approximation.  \n",
    "The method `count()` provides the rounded integer cardinality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49605a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperLogLog:\n",
    "    \n",
    "    def __init__(self, p=10, hash_name= \"sha1\"):\n",
    "        self.p = p\n",
    "        self.m = 2**p\n",
    "        self.M = [0] * self.m  # registers\n",
    "        self.alpha_m = self._alpha(self.m)\n",
    "        self.hash_name = hash_name\n",
    "    \n",
    "    @staticmethod\n",
    "    def _alpha(m):\n",
    "        if m == 16:  return 0.673\n",
    "        if m == 32:  return 0.697\n",
    "        if m == 64:  return 0.709\n",
    "        # m >= 128\n",
    "        return 0.7213 / (1.0 + 1.079 / m)\n",
    "    \n",
    "    def apply_hash(self, value):\n",
    "        if not isinstance(value, (bytes, bytearray)):\n",
    "            value = str(value).encode(\"utf-8\")\n",
    "        if self.hash_name == \"sha1\":\n",
    "            digest = hashlib.sha1(value).digest()\n",
    "        else:\n",
    "            digest = hashlib.sha256(value).digest()\n",
    "        return int.from_bytes(digest[:8], \"big\")\n",
    "    \n",
    "    def process_element(self,item):\n",
    "        hash = self.apply_hash(item)\n",
    "        bin_hash = bin(hash)[2:].zfill(64) # Converte para binário (sem '0b') e assegura 64 bits\n",
    "        indice_bits = bin_hash[:self.p]\n",
    "        resto_bits = bin_hash[self.p:]\n",
    "        j = int(indice_bits, 2)\n",
    "        zeros = len(resto_bits) - len(resto_bits.lstrip(\"0\"))\n",
    "        rho = zeros + 1  # +1 \n",
    "        self.M[j] = max(self.M[j], rho) #Atualizar a lista de buckets\n",
    "        \n",
    "    def estimate(self):\n",
    "        m = self.m\n",
    "        Z = sum((2.0 ** -value) for value in self.M)\n",
    "        H = m / Z\n",
    "        E = self.alpha_m * m * H\n",
    "        E = self.small_range_correction(E)\n",
    "        return E\n",
    "    \n",
    "    def small_range_correction(self, E):\n",
    "        m = self.m\n",
    "\n",
    "        if E <= 2.5 * m:\n",
    "            V = self.M.count(0)\n",
    "            if V > 0:\n",
    "                return m * math.log(m / V)\n",
    "\n",
    "        return E\n",
    "\n",
    "    \n",
    "    def count(self):\n",
    "        return int(round(self.estimate()))\n",
    "\n",
    "    def process_list(self,list):\n",
    "        for element in list:\n",
    "            self.process_element(element)                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a15554",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## 8. Analysis of Example 1 [3,0 valor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f7810c",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## 9. Analysis of Example 2 [3,0 valor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e821172",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## 10. Pros and cons of the approach [2,0 valor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60974d68",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## 11. Future improvements [2,0 valor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b1eb04",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div style=\"text-align: center;\">\n",
    "    <br><br>\n",
    "    <p style=\"font-size: 40px;\">References [1,0 valor]</p>\n",
    "</div>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7daa26",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (WSL)",
   "language": "python",
   "name": "wsl-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
